{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將會用到的函式庫import進來\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#導入資料集\n",
    "train_path = \"train.txt\"\n",
    "test_path = \"test.txt\"\n",
    "def getData(path):\n",
    "    text = []\n",
    "    label = []\n",
    "    with open(path) as f:\n",
    "        for i in f.readlines():\n",
    "            rows = i.split(\";\")\n",
    "            text.append(rows[0])\n",
    "            label.append(rows[1])\n",
    "    return text,label\n",
    "train_text, train_label = getData(train_path)\n",
    "test_text, test_label = getData(test_path)\n",
    "\n",
    "df = pd.DataFrame(zip(train_text, train_label),columns=[\"sentence\",\"label\"])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#空值檢查\n",
    "print(df.shape)\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#資料前處理\n",
    "\n",
    "def remove_hyperlinks(text):\n",
    "    sentence = re.sub(r\"http\",\"\",text)\n",
    "    sentence = re.sub(r\"www\",\"\",sentence)\n",
    "    return sentence\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return ''.join([word for word in text if word not in string.punctuation])\n",
    "def reshape(text):\n",
    "    sentence = re.sub(r\"\\n\",\"\",text)\n",
    "    return sentence\n",
    "\n",
    "df['sentence'] = df['sentence'].apply(lambda x: remove_hyperlinks(x.lower()))\n",
    "df['label'] = df['label'].apply(lambda x: reshape(x))\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分詞\n",
    "def tokenize(text):\n",
    "    return text.split(\" \")\n",
    "\n",
    "df['sentence'] = df['sentence'].apply(lambda x: remove_punctuation(x))\n",
    "df['sentence'] = df['sentence'].apply(lambda x: tokenize(x))\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#移除停用詞\n",
    "\n",
    "stopword = stopwords.words('english')\n",
    "def remove_stopword(text):\n",
    "    return [word for word in text if word not in stopword]\n",
    "\n",
    "df['sentence'] = df['sentence'].apply(lambda x: remove_stopword(x))\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#還原詞性\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text])\n",
    "\n",
    "df['sentence'] = df['sentence'].apply(lambda x: lemmatize(x))\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取得label列表\n",
    "category = df[\"label\"].unique().tolist()\n",
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取得text列表，並將dataframe中label的值轉為label列表中對應的index\n",
    "sentence = df['sentence'].values\n",
    "label = df[\"label\"].apply(lambda x: category.index(x)).values\n",
    "label[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取得所有不重複的詞彙\n",
    "words = [word.lower() for s in sentence for word in s.split(\" \")]\n",
    "various_words = list(set(words))\n",
    "various_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立字典索引，並交換key和value\n",
    "int2word = dict(enumerate(various_words))\n",
    "word2int = {w:int(i) for i,w in int2word.items()}\n",
    "word2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#計算每個text中詞彙的個數\n",
    "\n",
    "sentence_length = [len(s.split()) for s in sentence]\n",
    "counts = dict(Counter(sentence_length))\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#繪製詞彙長度分布圖\n",
    "plt.figure(figsize=(16,5))\n",
    "plt.bar(counts.keys(),counts.values())\n",
    "plt.xlabel(\"sentence_length\")\n",
    "plt.ylabel(\"num\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取得上下界限值的值及它的個數\n",
    "min_sen = min(counts.items())\n",
    "max_sen = max(counts.items())\n",
    "min_sen,max_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取得詞彙長度等於上下限的值之index\n",
    "min_index = [i for i,length in enumerate(sentence_length) if length==min_sen[0]]\n",
    "max_index = [i for i,length in enumerate(sentence_length) if length==max_sen[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#刪除最小值和最大值的文本\n",
    "new_text = np.delete(sentence, min_index)\n",
    "new_text2 = np.delete(new_text, max_index)\n",
    "\n",
    "new_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#刪除最大和最小值的標籤\n",
    "new_labels = np.delete(label, min_index)\n",
    "new_labels = np.delete(new_labels, max_index)\n",
    "\n",
    "new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#利用建立好的字典將詞彙token轉為數字\n",
    "text2ints = []\n",
    "for sentence in new_text2:\n",
    "    sample = list()\n",
    "    for word in sentence.split():\n",
    "        int_value = word2int[word]\n",
    "        sample.append(int_value)\n",
    "    text2ints.append(sample)\n",
    "text2ints[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將每一個句子padding到同樣的長度，過長的句子截斷\n",
    "def reset_text(text, seq_len):\n",
    "    dataset = np.zeros((len(text),seq_len))\n",
    "    for index,sentence in enumerate(text):\n",
    "        if len(sentence) < seq_len:\n",
    "            dataset[index, :len(sentence)] = sentence\n",
    "        else:\n",
    "            dataset[index, :] = sentence[:seq_len]\n",
    "    return dataset\n",
    "            \n",
    "dataset = reset_text(text2ints, seq_len=22)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把numpy矩陣轉為tensor張量\n",
    "dataset_tensor = torch.from_numpy(dataset)\n",
    "label_tensor = torch.from_numpy(new_labels)\n",
    "print(type(dataset_tensor), type(label_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#設定訓練和驗證資料集的比例\n",
    "all_samples = len(dataset_tensor)\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#從原始資料集中根據比例建立訓練資料集和驗證資料集\n",
    "train = dataset_tensor[:int(train_ratio*all_samples)]\n",
    "train_labels = label_tensor[:int(train_ratio*all_samples)]\n",
    "\n",
    "val = dataset_tensor[int(train_ratio*all_samples):]\n",
    "val_labels = label_tensor[int(train_ratio*all_samples):]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將文本和標籤打包成一個Dataset，並分別建立Dataloader\n",
    "\n",
    "train_dataset = TensorDataset(train, train_labels)\n",
    "val_dataset = TensorDataset(val, val_labels)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True, drop_last=True)\n",
    "for i,j in train_loader:\n",
    "    print(i,j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#若cuda能夠使用則使用gpu訓練，否則使用cpu\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立模型\n",
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, input_size, embedding_dim, hidden_dim, output_size, num_layers, dropout=0.5):\n",
    "        super(SentimentNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding= nn.Embedding(input_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, dropout=dropout,batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(128, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "\n",
    "    def forward(self, x, hidden):      \n",
    "        batch_size = x.size(0)\n",
    "        x = x.long() \n",
    "        embeds = self.embedding(x) # embeds(128,10,200)\n",
    "        \n",
    "        \n",
    "        out,hidden = self.lstm(embeds, hidden) \n",
    "        out = self.linear(out[:, -1, :]) # out(128，128)\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out) \n",
    "        out = self.linear2(out) # out(128, 6)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    #初始化隱藏層\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters())\n",
    "        # h_0, c_0\n",
    "        return (weight.new_zeros(self.num_layers, batch_size, self.hidden_dim),\n",
    "                weight.new_zeros(self.num_layers, batch_size, self.hidden_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定義參數\n",
    "input_size = len(word2int)\n",
    "output_size = len(category)\n",
    "print(output_size)\n",
    "embedding_dim = 200\n",
    "hidden_dim= 128\n",
    "num_layers= 2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立model\n",
    "model = SentimentNet(input_size, embedding_dim, hidden_dim, output_size, num_layers)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定義loss function、optimizer和scheduler\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立訓練和驗證function\n",
    "def train(model, data_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    train_process = dict()\n",
    "    train_loss_epoch10, val_loss_epoch10= [],[]\n",
    "    val_acc_epoch10 = []\n",
    "    for epoch in range(num_epochs):\n",
    "        hs = model.init_hidden(batch_size)\n",
    "        train_loss = []\n",
    "        train_correct = 0\n",
    "        model.train()\n",
    "        for data, target in data_loader:  \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output,hs = model(data, hs)\n",
    "            preds = torch.argmax(output, dim=1)\n",
    "            train_correct += torch.sum(preds==target)\n",
    "            \n",
    "            hs = tuple([h.data for h in hs])\n",
    "            loss = criterion(output, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "        print(f\"Epoch [{epoch}/{num_epochs-1}]---train loss {np.mean(train_loss):>.5f}\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            validation_loss, validation_acc = validation(model, val_loader, criterion)\n",
    "            \n",
    "            train_loss_epoch10.append(np.mean(train_loss))\n",
    "            val_loss_epoch10.append(validation_loss)\n",
    "            val_acc_epoch10.append(validation_acc)\n",
    "    \n",
    "    train_process[\"train_loss\"] = train_loss_epoch10\n",
    "    train_process[\"val_loss\"] = val_loss_epoch10\n",
    "    train_process[\"val_acc\"] = val_acc_epoch10\n",
    "    return train_process\n",
    "            \n",
    "def validation(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    hs = model.init_hidden(batch_size)\n",
    "    val_loss = []\n",
    "    val_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            outs,hs = model(data,hs)\n",
    "            hs = tuple([h.data for h in hs])\n",
    "        \n",
    "            loss = criterion(outs, target)\n",
    "            preds = torch.argmax(outs, dim=1)\n",
    "            val_loss.append(loss.item())\n",
    "            val_correct += torch.sum(preds==target)\n",
    "    print(f\"--------------------------------validation loss is: {np.mean(val_loss):>.5f}, acc is: {100*val_correct/len(val_loader.dataset):>.2f}%\")\n",
    "    return np.mean(val_loss), val_correct/len(val_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#開始訓練\n",
    "train_process = train(model, train_loader, criterion, optimizer,exp_lr_scheduler, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#繪製loss曲線和驗證準確度\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(train_process[\"train_loss\"],label=\"train-loss\")\n",
    "plt.plot(train_process[\"val_loss\"],label=\"val-loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.legend(labels=[\"\"])\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Validation Accuracy\")\n",
    "val_acc_cpu = [acc.cpu().numpy() for acc in train_process[\"val_acc\"]]\n",
    "plt.plot(val_acc_cpu)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用測試資料集預測\n",
    "preds_label = []\n",
    "def converts(text):\n",
    "    text = remove_hyperlinks(text)\n",
    "    new_text = remove_punctuation(text)\n",
    "    test_text_ints = [word2int[word.lower()]for word in new_text.split() if word in word2int.keys()]\n",
    "    return test_text_ints\n",
    "def predict(model):\n",
    "    correct = 0\n",
    "    test_text_int = [converts(text) for text in test_text]\n",
    "\n",
    "    new_test_text_int = reset_text(test_text_int, seq_len=22)\n",
    "    text_tensor = torch.from_numpy(new_test_text_int)\n",
    "\n",
    "    batch_size = text_tensor.size(0)\n",
    "    hs = model.init_hidden(batch_size)\n",
    "    \n",
    "    text_tensor = text_tensor.to(device)\n",
    "    outs, hs = model(text_tensor, hs)\n",
    "    preds = torch.argmax(outs, dim=1)\n",
    "\n",
    "    for i in range(len(test_text)):\n",
    "      print(test_text[i])\n",
    "      print(\" prediction: \", category[int(preds[i])])\n",
    "      preds_label.append(category[int(preds[i])])\n",
    "    for i in range(len(preds_label)):\n",
    "      test_label[i] = reshape(test_label[i])\n",
    "      if preds_label[i] == test_label[i]:\n",
    "          correct+=1\n",
    "    \n",
    "    print(\" test acc: \", (correct / len(preds_label)))\n",
    "\n",
    "\n",
    "\n",
    "predict(model)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSTM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
